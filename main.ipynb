{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZruTeWrolEh"
   },
   "source": [
    "# Object localization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports and GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_May__3_19:00:59_Pacific_Daylight_Time_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.64\n",
      "Build cuda_11.7.r11.7/compiler.31294372_0\n",
      "torch:  2.0 ; cuda:  2.0.1\n",
      "3.2.0\n",
      "GPU available: True\n",
      "GPU name: NVIDIA RTX A4000\n"
     ]
    }
   ],
   "source": [
    "import torch, mmdetection.mmdet as mmdet\n",
    "!nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "\n",
    "print(mmdet.__version__)\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"GPU available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enviroment inforamtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.platform: win32\n",
      "Python: 3.8.17 (default, Jul  5 2023, 20:35:33) [MSC v.1916 64 bit (AMD64)]\n",
      "CUDA available: True\n",
      "numpy_random_seed: 2147483648\n",
      "GPU 0: NVIDIA RTX A4000\n",
      "CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\n",
      "NVCC: Cuda compilation tools, release 11.7, V11.7.64\n",
      "MSVC: n/a, reason: fileno\n",
      "PyTorch: 2.0.1\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 193431937\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.15.2\n",
      "OpenCV: 4.7.0\n",
      "MMEngine: 0.8.3\n",
      "MMDetection: 3.2.0+\n"
     ]
    }
   ],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdetection.mmdet as mmdet\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download checkpoints (for finetuning, if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-23T06:50:31.399928Z",
     "iopub.status.busy": "2023-07-23T06:50:31.398995Z",
     "iopub.status.idle": "2023-07-23T06:51:07.755071Z",
     "shell.execute_reply": "2023-07-23T06:51:07.753888Z",
     "shell.execute_reply.started": "2023-07-23T06:50:31.399891Z"
    },
    "id": "Y57gZsC0FC7b",
    "outputId": "82ff324a-0461-44ae-ccc2-0d0d157fa0c2"
   },
   "outputs": [],
   "source": [
    "# !mkdir ./checkpoints\n",
    "# !mim download mmdet --config rtmdet-ins_l_8xb32-300e_coco --dest ./checkpoints\n",
    "# !mim download mmdet --config mask-rcnn_r50-caffe_fpn_ms-poly-3x_coco --dest ./checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert dataset to coco (bbox detection only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import mmcv\n",
    "import mmengine\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pycocotools.mask as maskUtils\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def is_clockwise(contour):\n",
    "    value = 0\n",
    "    num = len(contour)\n",
    "    for i, point in enumerate(contour):\n",
    "        p1 = contour[i]\n",
    "        if i < num - 1:\n",
    "            p2 = contour[i + 1]\n",
    "        else:\n",
    "            p2 = contour[0]\n",
    "        value += (p2[0][0] - p1[0][0]) * (p2[0][1] + p1[0][1]);\n",
    "    return value < 0\n",
    "\n",
    "def get_merge_point_idx(contour1, contour2):\n",
    "    idx1 = 0\n",
    "    idx2 = 0\n",
    "    distance_min = -1\n",
    "    for i, p1 in enumerate(contour1):\n",
    "        for j, p2 in enumerate(contour2):\n",
    "            distance = pow(p2[0][0] - p1[0][0], 2) + pow(p2[0][1] - p1[0][1], 2);\n",
    "            if distance_min < 0:\n",
    "                distance_min = distance\n",
    "                idx1 = i\n",
    "                idx2 = j\n",
    "            elif distance < distance_min:\n",
    "                distance_min = distance\n",
    "                idx1 = i\n",
    "                idx2 = j\n",
    "    return idx1, idx2\n",
    "\n",
    "def merge_contours(contour1, contour2, idx1, idx2):\n",
    "    contour = []\n",
    "    for i in list(range(0, idx1 + 1)):\n",
    "        contour.append(contour1[i])\n",
    "    for i in list(range(idx2, len(contour2))):\n",
    "        contour.append(contour2[i])\n",
    "    for i in list(range(0, idx2 + 1)):\n",
    "        contour.append(contour2[i])\n",
    "    for i in list(range(idx1, len(contour1))):\n",
    "        contour.append(contour1[i])\n",
    "    contour = np.array(contour)\n",
    "    return contour\n",
    "\n",
    "def merge_with_parent(contour_parent, contour):\n",
    "    if not is_clockwise(contour_parent):\n",
    "        contour_parent = contour_parent[::-1]\n",
    "    if is_clockwise(contour):\n",
    "        contour = contour[::-1]\n",
    "    idx1, idx2 = get_merge_point_idx(contour_parent, contour)\n",
    "    return merge_contours(contour_parent, contour, idx1, idx2)\n",
    "\n",
    "def mask2polygon(image):\n",
    "    contours, hierarchies = cv2.findContours(image, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "    contours_approx = []\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        epsilon = 0.001 * cv2.arcLength(contour, True)\n",
    "        contour_approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        contours_approx.append(contour_approx)\n",
    "\n",
    "    contours_parent = []\n",
    "    for i, contour in enumerate(contours_approx):\n",
    "        parent_idx = hierarchies[0][i][3]\n",
    "        if parent_idx < 0 and len(contour) >= 3:\n",
    "            contours_parent.append(contour)\n",
    "        else:\n",
    "            contours_parent.append([])\n",
    "\n",
    "    for i, contour in enumerate(contours_approx):\n",
    "        parent_idx = hierarchies[0][i][3]\n",
    "        if parent_idx >= 0 and len(contour) >= 3:\n",
    "            contour_parent = contours_parent[parent_idx]\n",
    "            if len(contour_parent) == 0:\n",
    "                continue\n",
    "            contours_parent[parent_idx] = merge_with_parent(contour_parent, contour)\n",
    "\n",
    "    contours_parent_tmp = []\n",
    "    for contour in contours_parent:\n",
    "        if len(contour) == 0:\n",
    "            continue\n",
    "        contours_parent_tmp.append(contour)\n",
    "\n",
    "    polygons = []\n",
    "    for contour in contours_parent_tmp:\n",
    "        polygon = contour.flatten().tolist()\n",
    "        polygons.append(polygon)\n",
    "    return polygons, contours\n",
    "\n",
    "def rle2polygon(segmentation, mask):\n",
    "    if isinstance(segmentation[\"counts\"], list):\n",
    "        segmentation = mask.frPyObjects(segmentation, *segmentation[\"size\"])\n",
    "    m = mask.decode(segmentation)\n",
    "    m[m > 0] = 255\n",
    "    polygons = mask2polygon(m)\n",
    "    return polygons\n",
    "\n",
    "def get_files_ending_with_ext(dir, ext='.jpg'):\n",
    "    collection = {}\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if file.endswith(ext):\n",
    "                collection[os.path.basename(file)[:-4]] = os.path.join(root, file)\n",
    "    return collection\n",
    "\n",
    "def vis_side_by_side(img, mask, title):\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.suptitle(title)\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(mask*50)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def hierarchical_cnt_to_yolo_fmt(contours, h, w):\n",
    "    contours = [np.array(cnt) for cnt in contours]\n",
    "    mask = np.zeros((h, w)).astype(np.uint8)\n",
    "    cv2.drawContours(mask, contours, -1, 1, thickness=cv2.FILLED)\n",
    "    ## convert mask to poly and preserve hierarchy in YOLO format\n",
    "    modified_polys, original_polys = mask2polygon(mask)\n",
    "    return modified_polys, original_polys\n",
    "\n",
    "\n",
    "\n",
    "def create_annotation_dict(ann_idx,\n",
    "                           ann_obj_count,\n",
    "                           ann_current_category_id,\n",
    "                           ann_abs_bbox, ann_abs_w,\n",
    "                           ann_abs_h):\n",
    "    \"\"\"\n",
    "    Create a dictionary structure for annotation data.\n",
    "\n",
    "    Parameters:\n",
    "    - ann_idx (int): The index of the annotation.\n",
    "    - ann_obj_count (int): A count of the annotation objects.\n",
    "    - ann_current_category_id (int): The ID of the current category.\n",
    "    - ann_abs_bbox (list): The bounding box.\n",
    "    - ann_abs_w (int): The width.\n",
    "    - ann_abs_h (int): The height.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the formatted annotation data.\n",
    "    \"\"\"\n",
    "\n",
    "    data_anno = dict(\n",
    "        image_id=ann_idx,\n",
    "        id=ann_obj_count,\n",
    "        category_id=ann_current_category_id,\n",
    "        bbox=ann_abs_bbox,\n",
    "        area=ann_abs_w * ann_abs_h,\n",
    "        iscrowd=0\n",
    "    )\n",
    "\n",
    "    return data_anno\n",
    "\n",
    "\n",
    "\n",
    "def seg_to_rle(contours_list, height, width):\n",
    "\n",
    "    # create an empty mask of the same size as the original image\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # fill the contours in the mask\n",
    "    cv2.drawContours(mask, contours_list, -1, 1, thickness=cv2.FILLED)\n",
    "\n",
    "    ## convert the binary mask to uncompressed RLE format\n",
    "    rle_uncompressed = maskUtils.encode(np.asfortranarray(mask))\n",
    "    rle_size = rle_uncompressed['size']\n",
    "\n",
    "    ## convert the RLE format to a compressed string\n",
    "    rle = maskUtils.encode(np.asfortranarray(mask))['counts'].decode('utf-8')\n",
    "\n",
    "    return rle, rle_size\n",
    "\n",
    "\n",
    "def create_mock_container_mask(bbox):\n",
    "    x, y, w, h = bbox\n",
    "\n",
    "    # calculate reduction % on each side\n",
    "    reduction = int(w * 0.15)\n",
    "\n",
    "    # create a smaller contour with the lower horizontal line shorter by reduction % from each side and centered\n",
    "    mock_contour = [\n",
    "        [x + 1, y + 1],  # Top-left\n",
    "        [x + w - 2, y + 1],  # Top-right\n",
    "        [x + w - reduction - 2, y + h - 2], # bottom-right (reduction % from the right)\n",
    "        [x + reduction + 1, y + h - 2] # bottom-left (reduction % from the left)\n",
    "    ]\n",
    "    \n",
    "    flat_mock_contour = [[item for sublist in mock_contour for item in sublist]]\n",
    "    \n",
    "    return flat_mock_contour\n",
    "\n",
    "\n",
    "def convert_dataset_to_coco(jsons_dir, out_file, subset):\n",
    "\n",
    "    \"\"\"\n",
    "    Convert dataset annotations in JSON format to COCO format.\n",
    "\n",
    "    Parameters:\n",
    "    - jsons_dir (str): The directory path containing JSON files of dataset annotations.\n",
    "    - out_file (str): The file path where the converted annotations in COCO format will be saved.\n",
    "    - subset (str): train/val subset identifier \n",
    "\n",
    "    Side effects:\n",
    "    - Writes a file: Outputs a file containing the annotations in COCO format at `out_file`.\n",
    "    \"\"\"\n",
    "\n",
    "    category_ids_dict = {\n",
    "        'cars': 0,\n",
    "        'dogs': 1,\n",
    "        'cats': 2,\n",
    "    }\n",
    "\n",
    "    # set the seed\n",
    "    random.seed(42)\n",
    "\n",
    "    # get the json files\n",
    "    json_files = []\n",
    "    for root, dirs, files in os.walk(jsons_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "    json_files = list(set(json_files))\n",
    "\n",
    "    # shuffle the list of JSON files\n",
    "    random.shuffle(json_files)\n",
    "\n",
    "    print('found {} json files'.format(len(json_files)))\n",
    "\n",
    "    annotations = []\n",
    "    images = []\n",
    "    obj_count = 0\n",
    "\n",
    "    for idx, json_file_path in tqdm(enumerate(json_files), desc=\"Processing JSON files\", ncols=100, total=len(json_files)):\n",
    "\n",
    "        with open(json_file_path) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        filename = os.path.join(data[\"image\"][\"file_path\"], data[\"image\"][\"file_name\"])\n",
    "\n",
    "        height, width = data['image']['height'], data['image']['width']\n",
    "\n",
    "        images.append(dict(\n",
    "            id=idx,\n",
    "            file_name=filename,\n",
    "            height=height,\n",
    "            width=width))\n",
    "\n",
    "        assert isinstance(data[\"annotations\"][\"bbox\"], list) or isinstance(data[\"annotations\"][\"container_bbox\"], list), 'Images must have at least one bbox.'\n",
    "\n",
    "        ###########################################################\n",
    "        ## get annotations for object\n",
    "        ###########################################################\n",
    "        if (subset not in ['val', 'train']) or not isinstance(data[\"annotations\"][\"bbox\"], list):            \n",
    "            pass\n",
    "        else:\n",
    "            assert len(data[\"annotations\"][\"bbox\"]) == 4, 'Invalid bbox format.'\n",
    "            \n",
    "            # get category ID (species class)\n",
    "            species_name = str(data[\"annotations\"][\"species\"]).lower()\n",
    "            if species_name in category_ids_dict:\n",
    "                current_category_id = category_ids_dict[species_name]\n",
    "            else:\n",
    "                raise Exception(f'Unknown species {data[\"annotations\"][\"species\"]} encountered.')\n",
    "\n",
    "            # get bbox\n",
    "            abs_x, abs_y, abs_w, abs_h = data['annotations'][\"bbox\"]\n",
    "            abs_bbox = [abs_x, abs_y, abs_w, abs_h]\n",
    "\n",
    "\n",
    "            # create the annotation dictionary\n",
    "            data_anno = create_annotation_dict(ann_idx=idx,\n",
    "                                               ann_obj_count=obj_count,\n",
    "                                               ann_current_category_id=current_category_id,\n",
    "                                               ann_abs_bbox=abs_bbox,\n",
    "                                               ann_abs_w=abs_w,\n",
    "                                               ann_abs_h=abs_h,\n",
    "                                              )\n",
    "            annotations.append(data_anno)\n",
    "            obj_count += 1\n",
    "\n",
    "\n",
    "        ###########################################################\n",
    "        ## get annotations for object 2\n",
    "        ###########################################################\n",
    "        # make sure there is bbox\n",
    "        if (subset not in ['val', 'train']) or (not isinstance(data[\"annotations\"][\"container_bbox\"], list)):\n",
    "            pass\n",
    "        else:\n",
    "            assert len(data[\"annotations\"][\"container_bbox\"]) == 4, 'Invalid bbox format.'\n",
    "\n",
    "            # get category ID\n",
    "            current_category_id = category_ids_dict['container']\n",
    "\n",
    "            # get bbox\n",
    "            abs_x, abs_y, abs_w, abs_h = data['annotations'][\"container_bbox\"]\n",
    "            abs_bbox = [abs_x, abs_y, abs_w, abs_h]\n",
    "\n",
    "\n",
    "            # create the annotation dictionary\n",
    "            data_anno = create_annotation_dict(ann_idx=idx,\n",
    "                                               ann_obj_count=obj_count,\n",
    "                                               ann_current_category_id=current_category_id,\n",
    "                                               ann_abs_bbox=abs_bbox,\n",
    "                                               ann_abs_w=abs_w,\n",
    "                                               ann_abs_h=abs_h,\n",
    "                                              )\n",
    "            annotations.append(data_anno)\n",
    "            obj_count += 1\n",
    "\n",
    "            \n",
    "    # get the categories\n",
    "    categories = [{'id': id, 'name': name} for name, id in category_ids_dict.items()]\n",
    "\n",
    "    # create the coco format json\n",
    "    coco_format_json = dict(\n",
    "        images=images,\n",
    "        annotations=annotations,\n",
    "        categories=categories\n",
    "    )\n",
    "\n",
    "    # save the coco format json\n",
    "    mmengine.dump(coco_format_json, out_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('WARNING - this loader is edited to not check segmentation json validation =accepted, since this exp is for bbox detection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up date dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_data_root = 'G:/Datasets/images'\n",
    "json_data_root = 'G:/Datasets/jsons'\n",
    "\n",
    "train_dataset_path = os.path.join(json_data_root, 'train')\n",
    "val_dataset_path = os.path.join(json_data_root, 'val')\n",
    "\n",
    "train_export_path = os.path.join(json_data_root, 'coco_train.json')\n",
    "val_export_path = os.path.join(json_data_root, 'coco_val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert data to coco format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dataset_to_coco(jsons_dir = train_dataset_path,\n",
    "                        out_file = train_export_path,\n",
    "                        subset = 'train')\n",
    "\n",
    "convert_dataset_to_coco(jsons_dir = val_dataset_path,\n",
    "                        out_file = val_export_path,\n",
    "                        subset = 'val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3va9IylvPEO"
   },
   "source": [
    "#### Checking the label corresponding to the instance split ID after the data format conversion is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-07-23T10:57:00.643910Z",
     "iopub.status.busy": "2023-07-23T10:57:00.643445Z",
     "iopub.status.idle": "2023-07-23T10:57:03.314073Z",
     "shell.execute_reply": "2023-07-23T10:57:03.312861Z",
     "shell.execute_reply.started": "2023-07-23T10:57:00.643874Z"
    },
    "id": "BCp7wAwbhOpB",
    "outputId": "88ee797b-63d7-47e3-991f-50533f54477a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# path to load the COCO annotation file\n",
    "annotation_file = val_export_path\n",
    "\n",
    "# initialise the COCO object\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# get all category tags and corresponding category IDs\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "category_id_to_name = {cat['id']: cat['name'] for cat in categories}\n",
    "\n",
    "# print all category IDs and corresponding category names\n",
    "for category_id, category_name in category_id_to_name.items():\n",
    "    print(f\"Category ID: {category_id}, Category Name: {category_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize the dataset to make sure things are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import mmcv\n",
    "import os\n",
    "\n",
    "# initialize the COCO object\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# load an image and its annotations\n",
    "for img_id in range(10):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "    # load the image from a local file\n",
    "    img_path = os.path.join(image_data_root, img_info['file_name'])\n",
    "    img = mmcv.imread(img_path)\n",
    "\n",
    "    # create a matplotlib figure and axis\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # draw bounding boxes\n",
    "    for ann in anns:\n",
    "        bbox = ann['bbox']\n",
    "        rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    print('img_path', img_path)\n",
    "    plt.axis('off')\n",
    "    plt.title(img_path)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7gIR2Z4vVj8"
   },
   "source": [
    "#### initiate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T10:31:26.426003Z",
     "iopub.status.busy": "2023-07-23T10:31:26.425605Z",
     "iopub.status.idle": "2023-07-23T10:31:27.757978Z",
     "shell.execute_reply": "2023-07-23T10:31:27.756964Z",
     "shell.execute_reply.started": "2023-07-23T10:31:26.425972Z"
    },
    "id": "Xz_MtfREILha",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "from mmengine.runner import set_random_seed\n",
    "from pprint import pprint\n",
    "\n",
    "# config file\n",
    "cfg = Config.fromfile('./mmdetection/configs/faster_rcnn/faster-rcnn_r50_fpn_amp-1x_coco.py')\n",
    "\n",
    "# pre-training weight paths.\n",
    "cfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_fp16_1x_coco_20200204-d4dc1471.pth'\n",
    "\n",
    "# working folder\n",
    "cfg.work_dir = './work_dir'\n",
    "\n",
    "# max epochs\n",
    "cfg.max_epochs = 100\n",
    "\n",
    "# batch size\n",
    "cfg.train_dataloader.batch_size = 4\n",
    "\n",
    "# dataloader num workers\n",
    "cfg.train_dataloader.num_workers = 8\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training. We divide it by 8 since we only use one GPU.\n",
    "change_in_batch_size = cfg.train_dataloader.batch_size / 2 # added this to account for our batch size, default in mmdet is 2 per gpu, when using batch of 4, means we have size bigger by a factor of 2 times.\n",
    "cfg.optim_wrapper.optimizer.lr = (0.02 / 8) * change_in_batch_size\n",
    "\n",
    "# metainfo \n",
    "cfg.metainfo = {\n",
    "    'classes': ('cars', 'dogs', 'cats'),\n",
    "    'palette': [\n",
    "        (220, 20, 60),    # RGB color for class 'cars' (red-ish)\n",
    "        (65, 105, 225),   # RGB color for class 'dogs' (blue-ish)\n",
    "        (34, 139, 34),    # RGB color for class 'cats' (green-ish)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# data folder\n",
    "cfg.data_root = image_data_root\n",
    "\n",
    "# train json file path\n",
    "cfg.train_dataloader.dataset.ann_file = './dataset/train.json'\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "\n",
    "# train image file path\n",
    "cfg.train_dataloader.dataset.data_prefix.img = '' # we dont need prefix\n",
    "\n",
    "# update metainfo\n",
    "cfg.train_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "# valid json file path\n",
    "cfg.val_dataloader.dataset.ann_file = './dataset/val.json'\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "\n",
    "# valid image file path\n",
    "cfg.val_dataloader.dataset.data_prefix.img = ''\n",
    "cfg.val_dataloader.dataset.metainfo = cfg.metainfo\n",
    "\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "\n",
    "# valid evaluator json file path\n",
    "cfg.val_evaluator.ann_file = cfg.data_root + './dataset/val.json'\n",
    "\n",
    "cfg.val_evaluator.metric = ['bbox']\n",
    "\n",
    "cfg.test_evaluator = cfg.val_evaluator\n",
    "\n",
    "cfg.test_evaluator.classwise = True\n",
    "\n",
    "# model weights are saved every interval, up to two weights are saved at the same time, and the saving strategy is auto.\n",
    "cfg.default_hooks.checkpoint = dict(type='CheckpointHook', interval=3, max_keep_ckpts=2, save_best='auto')\n",
    "\n",
    "# interval of reporting indicators\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "\n",
    "cfg.train_cfg.max_epochs = cfg.max_epochs\n",
    "\n",
    "# evaluation of the model begins at epoch n\n",
    "cfg.train_cfg.val_begin = 10\n",
    "\n",
    "# evaluate the model every n epochs \n",
    "cfg.train_cfg.val_interval = 10 \n",
    "\n",
    "# dataset loader :\n",
    "cfg.train_dataloader.dataset = cfg.train_dataloader.dataset\n",
    "\n",
    "# fixed random number seed\n",
    "set_random_seed(0, deterministic=False)\n",
    "\n",
    "# we can also use tensorboard to log the training process (optional)\n",
    "cfg.visualizer.vis_backends.append({\"type\":'TensorboardVisBackend'})\n",
    "\n",
    "# Modify num classes of the model in box head and mask head\n",
    "cfg.model.roi_head.bbox_head.num_classes = 9\n",
    "\n",
    "config=f'./mmdetection/configs/custom_detector_config.py'\n",
    "with open(config, 'w') as f:\n",
    "    f.write(cfg.pretty_text)\n",
    "\n",
    "pprint(vars(cfg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import logging\n",
    "import os\n",
    "import os.path as osp\n",
    "from mmengine.config import Config, DictAction\n",
    "from mmengine.logging import print_log\n",
    "from mmengine.registry import RUNNERS\n",
    "from mmengine.runner import Runner\n",
    "from mmdetection.mmdet.utils import setup_cache_size_limit_of_dynamo\n",
    "\n",
    "# Instead of parsing args, we will use this configuration class\n",
    "class ConfigArgs:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = config_path\n",
    "        self.work_dir = None\n",
    "        self.amp = False\n",
    "        self.auto_scale_lr = False\n",
    "        self.resume = None\n",
    "        self.cfg_options = None\n",
    "        self.launcher = 'none'\n",
    "        self.local_rank = 0\n",
    "\n",
    "\n",
    "def trigger_training(config_path):\n",
    "    args = ConfigArgs(config_path)\n",
    "    \n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    # Reduce the number of repeated compilations and improve training speed.\n",
    "    setup_cache_size_limit_of_dynamo()\n",
    "\n",
    "    # load config\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    cfg.launcher = args.launcher\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    # determine work_dir\n",
    "    if args.work_dir is not None:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif cfg.get('work_dir', None) is None:\n",
    "        ## if not in cfg\n",
    "        cfg.work_dir = osp.join('./work_dir',\n",
    "                                osp.splitext(osp.basename(args.config))[0])\n",
    "\n",
    "    # enable automatic-mixed-precision training\n",
    "    if args.amp is True:\n",
    "        optim_wrapper = cfg.optim_wrapper.type\n",
    "        if optim_wrapper == 'AmpOptimWrapper':\n",
    "            print_log(\n",
    "                'AMP training is already enabled in your config.',\n",
    "                logger='current',\n",
    "                level=logging.WARNING)\n",
    "        else:\n",
    "            assert optim_wrapper == 'OptimWrapper', (\n",
    "                '`--amp` is only supported when the optimizer wrapper type is '\n",
    "                f'`OptimWrapper` but got {optim_wrapper}.')\n",
    "            cfg.optim_wrapper.type = 'AmpOptimWrapper'\n",
    "            cfg.optim_wrapper.loss_scale = 'dynamic'\n",
    "\n",
    "    # enable automatically scaling LR\n",
    "    if args.auto_scale_lr:\n",
    "        if 'auto_scale_lr' in cfg and \\\n",
    "                'enable' in cfg.auto_scale_lr and \\\n",
    "                'base_batch_size' in cfg.auto_scale_lr:\n",
    "            cfg.auto_scale_lr.enable = True\n",
    "        else:\n",
    "            raise RuntimeError('Can not find \"auto_scale_lr\" or '\n",
    "                               '\"auto_scale_lr.enable\" or '\n",
    "                               '\"auto_scale_lr.base_batch_size\" in your'\n",
    "                               ' configuration file.')\n",
    "\n",
    "    # resume is determined in this priority: resume from > auto_resume\n",
    "    if args.resume == 'auto':\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = None\n",
    "    elif args.resume is not None:\n",
    "        cfg.resume = True\n",
    "        cfg.load_from = args.resume\n",
    "\n",
    "    \n",
    "    # build the runner from config\n",
    "    if 'runner_type' not in cfg:\n",
    "        runner = Runner.from_cfg(cfg)\n",
    "    else:\n",
    "        runner = RUNNERS.build(cfg)\n",
    "\n",
    "    # start training\n",
    "    runner.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trigger_training(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import os\n",
    "import os.path as osp\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from mmengine import ConfigDict\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmdet.engine.hooks.utils import trigger_visualization_hook\n",
    "from mmdet.evaluation import DumpDetResults\n",
    "from mmdet.registry import RUNNERS\n",
    "from mmdet.utils import setup_cache_size_limit_of_dynamo\n",
    "\n",
    "# Configuration class\n",
    "class ConfigArgs:\n",
    "    def __init__(self, config_path, checkpoint_path, out):\n",
    "        self.config = config_path\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.work_dir = None\n",
    "        self.out = out #None\n",
    "        self.show = False\n",
    "        self.show_dir = None\n",
    "        self.wait_time = 2\n",
    "        self.cfg_options = None\n",
    "        self.launcher = 'none'\n",
    "        self.tta = False\n",
    "        self.local_rank = 0\n",
    "\n",
    "def trigger_testing(config_path, checkpoint_path, out):\n",
    "    args = ConfigArgs(config_path, checkpoint_path, out)\n",
    "    \n",
    "    if 'LOCAL_RANK' not in os.environ:\n",
    "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
    "\n",
    "    # reduce the number of repeated compilations and improve testing speed.\n",
    "    setup_cache_size_limit_of_dynamo()\n",
    "\n",
    "    # load config\n",
    "    cfg = Config.fromfile(args.config)\n",
    "    cfg.launcher = args.launcher\n",
    "    if args.cfg_options is not None:\n",
    "        cfg.merge_from_dict(args.cfg_options)\n",
    "\n",
    "    # determine work_dir\n",
    "    if args.work_dir:\n",
    "        cfg.work_dir = args.work_dir\n",
    "    elif not cfg.get('work_dir'):\n",
    "        cfg.work_dir = osp.join('./work_dir', osp.splitext(osp.basename(args.config))[0])\n",
    "\n",
    "    cfg.load_from = args.checkpoint\n",
    "\n",
    "    if args.show or args.show_dir:\n",
    "        cfg = trigger_visualization_hook(cfg, args)\n",
    "\n",
    "    if args.tta:\n",
    "        if 'tta_model' not in cfg:\n",
    "            warnings.warn('Cannot find ``tta_model`` in config, we will set it as default.')\n",
    "            cfg.tta_model = dict(type='DetTTAModel', tta_cfg=dict(nms=dict(type='nms', iou_threshold=0.5), max_per_img=100))\n",
    "        if 'tta_pipeline' not in cfg:\n",
    "            warnings.warn('Cannot find ``tta_pipeline`` in config, we will set it as default.')\n",
    "            test_data_cfg = cfg.test_dataloader.dataset\n",
    "            while 'dataset' in test_data_cfg:\n",
    "                test_data_cfg = test_data_cfg['dataset']\n",
    "            cfg.tta_pipeline = deepcopy(test_data_cfg.pipeline)\n",
    "            flip_tta = dict(\n",
    "                type='TestTimeAug',\n",
    "                transforms=[\n",
    "                    [dict(type='RandomFlip', prob=1.), dict(type='RandomFlip', prob=0.)],\n",
    "                    [dict(type='PackDetInputs', meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor', 'flip', 'flip_direction'))],\n",
    "                ])\n",
    "            cfg.tta_pipeline[-1] = flip_tta\n",
    "        cfg.model = ConfigDict(**cfg.tta_model, module=cfg.model)\n",
    "        cfg.test_dataloader.dataset.pipeline = cfg.tta_pipeline\n",
    "\n",
    "    # build the runner from config\n",
    "    if 'runner_type' not in cfg:\n",
    "        runner = Runner.from_cfg(cfg)\n",
    "    else:\n",
    "        runner = RUNNERS.build(cfg)\n",
    "\n",
    "    if args.out:\n",
    "        assert args.out.endswith(('.pkl', '.pickle')), 'The dump file must be a pkl file.'\n",
    "        runner.test_evaluator.metrics.append(DumpDetResults(out_file_path=args.out))\n",
    "\n",
    "    # start testing\n",
    "    runner.test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trigger test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trigger_testing(config, \"./work_dir/best_coco_bbox_mAP_epoch_90.pth\", out = './results_file.pkl')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:mm_detection_env] *",
   "language": "python",
   "name": "conda-env-mm_detection_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
